{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002547bc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# https://rise.readthedocs.io/en/latest/\n",
    "# https://stackoverflow.com/questions/33647774/how-to-include-two-pictures-side-by-side-in-markdown-for-ipython-notebook-jupyt\n",
    "# https://www.markdownguide.org/basic-syntax/#code-blocks\n",
    "# https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax#images\n",
    "\n",
    "# https://docs.sciml.ai/SciMLSensitivity/stable/tutorials/parameter_estimation_ode/\n",
    "# https://docs.sciml.ai/Overview/stable/showcase/missing_physics/#autocomplete\n",
    "# https://fholtorf.github.io/MarkovBounds.jl/dev/tutorials/optimal_control/\n",
    "\n",
    "# https://www.youtube.com/playlist?list=PLP8iPy9hna6SOxXv_t1s8eEO6O0jTblqX\n",
    "# https://docs.sciml.ai/Overview/stable/showcase/missing_physics/#autocomplete\n",
    "# https://augustinas1.github.io/MomentClosure.jl/dev/tutorials/parameter_estimation_SDE/\n",
    "\n",
    "# outline of talk:\n",
    "\n",
    "# - title slide \n",
    "# - collaborators slide\n",
    "# - Solve forward problems \n",
    "#  - ODEs\n",
    "#  - SDEs¬Æ\n",
    "#  - Hybrid DEs\n",
    "#   - DDEs\n",
    "#   - but also other solvers: LinearSolve, NonlinearSolve.jl, DiscreteTime\n",
    "# - Inverse problems (parameter estimation, system identification, and optimal control)\n",
    "# - Motivation: Why we want to do gradient descent?\n",
    "# - What we need to do gradient descent through ODEs\n",
    "# - Where one needs to be careful (hybrid DEs, chaos, StochasticAD)\n",
    "# - Parameter estimation example (LV)\n",
    "# - UDE and system discovery (LV)\n",
    "# - Stochastic optimal control (LV)\n",
    "# - Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66a950",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Integrating Machine Learning into Differential Equations for Automated Discovery of Missing Physics and Optimal Control\n",
    "\n",
    "**Frank Sch√§fer** \\\n",
    "**Julia Lab** \\\n",
    "**Computer Science and Artifical Intelligence Laboratory** \\\n",
    "**MIT**\n",
    "\n",
    "| <img src=\"CSAIL_logo.png\" alt=\"CSAIL\" style=\"width: 200px;\"/> |  | <img src=\"MIT_logo.png\" alt=\"MIT\" style=\"width: 200px;\"/>|  | <img src=\"julia_logo.png\" alt=\"julia\" style=\"width: 200px;\"/>|\n",
    "|-|-|-|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24a8ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collaborators\n",
    "\n",
    "### SciML team\n",
    "- https://github.com/orgs/SciML/people \n",
    "- Chris Rackauckas\n",
    "- Yingbo Ma\n",
    "- Julius Martensen\n",
    "\n",
    "###  [Julia Lab](https://julia.mit.edu/) and other collaborators\n",
    "- Julian Arnold, Gaurav Arya, Christoph Bruder, Vaibhav Dixit, Alan Edelman, Flemming Holtorf, Niels L√∂rch, Avik Pal, Moritz Schauer ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389b59d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What to expect in this presentation\n",
    "\n",
    "### Explore techniques in *scientific machine learning*, including\n",
    "- Universal differential equations (UDEs)\n",
    "- Automatic differentiation (AD)\n",
    "- Sparse identification of nonlinear dynamics (SINDy)\n",
    "\n",
    "### Large set of involved *packages*:\n",
    "- SciML tools (DifferentialEquations.jl, SciMLSensitivity.jl, DataDrivenDiffEq.jl, ‚Ä¶)\n",
    "- Machine learning tools (Flux.jl, Lux.jl, ‚Ä¶)\n",
    "- Differentiation tools (FiniteDiff.jl, ForwardDiff.jl, Enzyme,jl, Zygote.jl, ReverseDiff.jl, SparseDiffTools.jl, AbstractDifferentiation.jl, ‚Ä¶)\n",
    "- Optimization tools (JuMP, Optim.jl, Optimisers.jl, NLopt.jl, ‚Ä¶)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c6756",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "pkg\"activate @DigiWell\"\n",
    "# pkg\"add DifferentialEquations, DataDrivenDiffEq, DataDrivenSparse, SciMLSensitivity\" \n",
    "# pkg\"add Optimization, OptimizationOptimisers, OptimizationOptimJL, LinearAlgebra, Statistics\"\n",
    "# pkg\"add ComponentArrays, Lux, Zygote, FiniteDiff, ForwardDiff, Plots, StableRNGs, QuadGK, LaTeXStrings, StochasticAD, Distributions\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6bcbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciML Tools\n",
    "using DifferentialEquations, DataDrivenDiffEq, SciMLSensitivity, DataDrivenSparse\n",
    "using Optimization, OptimizationOptimisers, OptimizationOptimJL \n",
    "\n",
    "# Standard Libraries\n",
    "using LinearAlgebra, Statistics\n",
    "\n",
    "# External Libraries\n",
    "using ComponentArrays, Lux, Zygote, FiniteDiff, ForwardDiff, Plots, LaTeXStrings, StableRNGs, QuadGK, StochasticAD, Distributions\n",
    "\n",
    "# Set a random seed for reproducible behaviour\n",
    "rng = StableRNG(4578478478) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69db5c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources:\n",
    "- Tutorial on [Automated Discovery of Missing Physics by Embedding Machine Learning into Differential Equations](https://docs.sciml.ai/Overview/stable/showcase/missing_physics/#autocomplete)\n",
    "- [SciMLSensitivity documentation](https://docs.sciml.ai/SciMLSensitivity/stable/)\n",
    "- Chris' JuliaCon 2020 workshop on [Doing Scientific Machine Learning (SciML) With Julia](https://www.youtube.com/watch?v=QwVO0Xh2Hbg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a56b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predator-prey model\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d\\text{üêá}}{dt} &= \\alpha \\text{üêá} - \\beta \\text{üêá} \\text{üê∫} \\\\\n",
    "\\frac{d\\text{üê∫}}{dt} &= -\\delta \\text{üê∫} + \\gamma \\text{üêá} \\text{üê∫}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "- üêá represents the population of prey.\n",
    "- üê∫ represents the population of predators.\n",
    "- Œ±, Œ≤, Œ≥, and Œ¥ are parameters governing the interactions between the prey and predator populations.\n",
    "\n",
    "... with possible modifications, such as\n",
    "\n",
    "- stochasticity \n",
    "- events\n",
    "- delays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a9d6a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Goals\n",
    "\n",
    "Demonstrate\n",
    "\n",
    "- Parameter inference\n",
    "- Model selection & discovery\n",
    "- Stochastic optimal control\n",
    "\n",
    "for variations of the predator-prey model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583f8caa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85d169",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "function lotka(u, p, t)\n",
    "    # right-hand-side of Lotka-Volterra system\n",
    "    # inputs:\n",
    "    #       - u : current state u[1] = population of prey,\n",
    "    #                           u[2] = population of predator\n",
    "    #       - p : parameters\n",
    "    # returns:\n",
    "    #       - du/dt : rate of change of population sizes\n",
    "\n",
    "    üêá, üê∫ = u\n",
    "    Œ±, Œ≤, Œ≥, Œ¥ = p\n",
    "    düêá = Œ± * üêá - Œ≤ * üêá * üê∫\n",
    "    düê∫ = - Œ¥ * üê∫ + Œ≥ * üêá * üê∫\n",
    "    return [düêá, düê∫]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b95f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the experimental parameters\n",
    "\n",
    "tspan = (0.0, 5.0) # time span\n",
    "u0 = [3.0, 1.5] # initial condition\n",
    "p_ = [1.3, 0.9, 0.8, 1.8] # parameters\n",
    "\n",
    "# Define ODE problem\n",
    "prob = ODEProblem(lotka, u0, tspan, p_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba1c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "u0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfeaa05-966a-426d-90f3-91e71e837a74",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solving the forward problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac693dc-1e0b-4ffd-903f-28c61dea22e7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "solve(prob, Tsit5())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318cae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea845dd0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Handling stochasticity, explicit and implicit events, time delays, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fa4bd",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Stochasticity\n",
    "lotka_noise(u, p, t) = [p[5] * u[1], p[6] * u[2]]\n",
    "prob_SDE = SDEProblem(lotka, lotka_noise, u0, tspan, [p_..., 0.1, 0.0])\n",
    "solution_SDE = [Array(solve(prob_SDE, SOSRI())(0.0:1.0:10.0)) for i in 1:20] # 20 solution samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a3d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events\n",
    "üî•üê∫_condition(u, t, integrator) = u[2] - 1.8\n",
    "üî•üê∫_affect!(integrator) = integrator.u[2] -= 0.2\n",
    "üî•üê∫_cb = ContinuousCallback(üî•üê∫_condition, üî•üê∫_affect!)\n",
    "plot(solve(prob, Vern7(), callback=üî•üê∫_cb, abstol=1e-12, reltol=1e-12, saveat=0.25), xlabel=\"time\", ylabel=\"population size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3f6ca",
   "metadata": {},
   "source": [
    "- [Stochastic differential equations](https://docs.sciml.ai/DiffEqDocs/stable/tutorials/sde_example/)\n",
    "- [Hybrid differential equations](https://docs.sciml.ai/DiffEqDocs/stable/features/callback_functions/)\n",
    "- [Delay differential equations](https://docs.sciml.ai/DiffEqDocs/stable/tutorials/dde_example/)\n",
    "- [Parallel ensemble simulations](https://docs.sciml.ai/DiffEqDocs/dev/features/ensemble/)\n",
    "\n",
    "... and some very recent developments:\n",
    "\n",
    "- [Solving linear systems](https://docs.sciml.ai/LinearSolve/stable/)\n",
    "- [Solving nonlinear systems](https://docs.sciml.ai/NonlinearSolve/stable/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f715d5-f37c-40ad-9929-47a6616e314b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Formulate inverse design task as an optimization problem\n",
    "\n",
    "- Define loss function and constraints\n",
    "- Compute gradients (`solve` is compatible with AD!)\n",
    "- Use gradient-based optimization methods to optimize (free) parameters \n",
    "\n",
    "- (Find a sparse symbolic representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b9cb9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generating the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363a576",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Add noise in terms of the mean\n",
    "X = Array(solution)\n",
    "t = solution.t\n",
    "\n",
    "xÃÑ = mean(X, dims=2)\n",
    "noise_magnitude = 5e-3\n",
    "Xdata = X .+ (noise_magnitude * xÃÑ) .* randn(rng, eltype(X), size(X))\n",
    "\n",
    "plot(solution, alpha=0.75, color=:black, label=[\"true data\" nothing], xlabel=\"time\", ylabel=\"population size\")\n",
    "scatter!(t, transpose(Xdata), color=:red, label=[\"noisy data\" nothing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f963218-ef7a-45a1-af00-b1bc47b0bd11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a predictor\n",
    "function predict(Œ∏)\n",
    "    _prob = remake(prob, p=Œ∏)\n",
    "    _sol = solve(_prob, Vern7(), saveat=t, abstol=1e-6, reltol=1e-6)\n",
    "    Array(_sol)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d4624-7422-47ec-bdc2-5edc4aaa5a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple L2 loss\n",
    "function loss(Œ∏)\n",
    "    XÃÇ = predict(Œ∏)\n",
    "    sum(abs2, Xdata .- XÃÇ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73433a22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are derivatives good for?\n",
    "\n",
    "- **sensitivity analysis**\n",
    "    - How sensitive is the solution to changes in the initial conditions/parameters?\n",
    "- **numerical methods**\n",
    "    - e.g. Hamiltonian Monte Carlo\n",
    "- **parameter inference & control**\n",
    "    - What parameters match the observed data?\n",
    "    - How can I drive the solution to a certain state?\n",
    "    \n",
    "    \n",
    "**$\\rightarrow$ easy when analytical solution is available! here: assume only access to a computer program**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd367e-383b-4804-9fc7-4839737312a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Different   sensitivity methods\n",
    "p0 = [1.0, 0.5, 1.2, 1.6]\n",
    "FiniteDiff.finite_difference_gradient(loss, p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579560d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.gradient(loss, p0) # forward-mode AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zygote.gradient(loss, p0)[1] # reverse-mode AD "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb9bd8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A lot is happening under the hood!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a769fc1",
   "metadata": {},
   "source": [
    "## Different modes of sensitivity analysis for DEs\n",
    "\n",
    "2 main ways of differentiating solutions of DEs:\n",
    "\n",
    "- **discretize-then-differentiate** (AD on solver operations) \\\n",
    "  ‚Äúexact gradient of the approximation‚Äù\n",
    "  \n",
    "- **differentiate-then-discretize** (custom rules) \\\n",
    "  ‚Äúapproximation of the exact gradient‚Äù\n",
    "\n",
    "... with two modes each (forward/tangent and reverse/adjoint)\\\n",
    "  $\\rightarrow$ optimal choice depends on number of states/parameters and system properties\n",
    "  \n",
    "No method fits all purposes!\n",
    "\n",
    "- small number of parameters (<100) $\\rightarrow$ forward mode\n",
    "- large number of parameters $\\rightarrow$ reverse mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d07ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Options in SciML\n",
    "\n",
    "| Method              | Stability     | Performance Regime | Memory Usage    | GPU-compatible |\n",
    "|---------------------|-------------- |--------------------|---------------- |----------------|\n",
    "| `BacksolveAdjoint`| Poor | Good performance on special non-stiff DEs | Low. O(1)    | Yes |\n",
    "| `InterpolatingAdjoint` | Good | Good performance on most ODEs | High. Requires full continuous solution of forward | Yes|\n",
    "| `QuadratureAdjoint` | Good | Good performance on sufficiently small DEs which are not memory-bound | Highest. Requires full continuous solution of forward and adjoint | No  |\n",
    "| `BacksolveAdjoint` (checkpointed)| Okay | Good performance on special non-stiff DEs | Medium. O(c) where c is the number of checkpoints | Yes  |\n",
    "| `InterpolatingAdjoint`(checkpointed) | Good        | Good                | Medium. O(c) where c is the number of checkpoints        | Yes            |\n",
    "| `ReverseDiffAdjoint`   | Best        | Good             | High. Requires full forward and reverse AD of solve | No (alternative: `TrackerAdjoint`) |\n",
    "\n",
    "- Very soon: `GaussAdjoint`, similar to `QuadratureAdjoint` but GPU compatible and memory efficient! \\\n",
    "(Joint work with Alex Cohen, Avik Pal, and Chris Rackauckas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb3938",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameter inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = Float64[]\n",
    "\n",
    "callback = function (p, l)\n",
    "    push!(losses, l)\n",
    "    if length(losses) % 50 == 0\n",
    "        println(\"Current loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "    end\n",
    "    return false\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adtype = Optimization.AutoZygote()\n",
    "optf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)\n",
    "optprob = Optimization.OptimizationProblem(optf, p0)\n",
    "\n",
    "# Train with Adam\n",
    "res1 = Optimization.solve(optprob, ADAM(), callback=callback, maxiters=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beccaa67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# true p arame ters\n",
    "p_   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cedadf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model selection & discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf34126",
   "metadata": {},
   "source": [
    "### Universal Differential Equations (UDEs)\n",
    "\n",
    "- use a neural network (NN) as a universal function approximator inside a DE  \n",
    "\n",
    "### Modified predator-prey model\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d\\text{üêá}}{dt} &= \\alpha \\text{üêá} + NN_\\theta(\\text{üêá}, \\text{üê∫})[1] \\\\\n",
    "\\frac{d\\text{üê∫}}{dt} &= -\\delta \\text{üê∫} + NN_\\theta(\\text{üêá}, \\text{üê∫})[2]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "- üêá represents the population of prey.\n",
    "- üê∫ represents the population of predators.\n",
    "- Œ± and Œ¥ as before, $NN_\\theta$ with parameters $\\theta$ to model unknown interaction between predator and prey.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35615a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural networks in Julia using [Lux.jl](https://lux.csail.mit.edu/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf(x) = exp.(-(x .^ 2))\n",
    "\n",
    "# Multilayer FeedForward\n",
    "const U = Lux.Chain(Lux.Dense(2, 5, rbf), Lux.Dense(5, 5, rbf), Lux.Dense(5, 5, rbf),\n",
    "              Lux.Dense(5, 2))\n",
    "# Get the initial parameters and state variables of the model\n",
    "p, st = Lux.setup(rng, U)\n",
    "const _st = st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40919093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UDE model\n",
    "function ude_dynamics!(du, u, p, t, p_true)\n",
    "    uÃÇ = U(u, p, _st)[1] # neural network prediction\n",
    "    du[1] = p_true[1] * u[1] + uÃÇ[1]\n",
    "    du[2] = -p_true[4] * u[2] + uÃÇ[2]\n",
    "end\n",
    "\n",
    "# Closure with the known parameter\n",
    "ude_dynamics!(du, u, p, t) = ude_dynamics!(du, u, p, t, p_)\n",
    "# Define the problem\n",
    "prob_nn = ODEProblem(ude_dynamics!, u0, tspan, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b4e4c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up the training loop\n",
    "function predict(Œ∏)\n",
    "    _prob = remake(prob_nn, p = Œ∏)\n",
    "    _sol = solve(_prob, Vern7(), saveat = t, abstol = 1e-6, reltol = 1e-6, sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true)))\n",
    "    Array(_sol)\n",
    "end\n",
    "\n",
    "function loss(Œ∏)\n",
    "    XÃÇ = predict(Œ∏)\n",
    "    mean(abs2, Xdata .- XÃÇ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed6922",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Run the training process!\n",
    "losses = Float64[]\n",
    "\n",
    "adtype = Optimization.AutoZygote()\n",
    "optf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)\n",
    "optprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p))\n",
    "\n",
    "\n",
    "res1 = Optimization.solve(optprob, ADAM(), callback = callback, maxiters = 5000)\n",
    "println(\"Training loss after $(length(losses)) iterations: $(losses[end])\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd7983",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Common strategy:\n",
    "\n",
    "- use the optimization result of the first run with Adam as the initial condition of a second optimization with (L)BFGS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optprob2 = Optimization.OptimizationProblem(optf, res1.u)\n",
    "res2 = Optimization.solve(optprob2, Optim.LBFGS(), callback = callback, maxiters = 1000)\n",
    "println(\"Final training loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "\n",
    "# Rename the best candidate\n",
    "p_trained = res2.u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca6e35",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualizing the trained UDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "pl_losses = plot(1:5000, losses[1:5000], yaxis = :log10, xaxis = :log10, label = \"ADAM\", color = :blue)\n",
    "plot!(5001:length(losses), losses[5001:end], yaxis = :log10, xaxis = :log10,\n",
    "      xlabel = \"iterations\", ylabel = \"loss\", label = \"BFGS\", color = :red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50531c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis of the trained network\n",
    "# Plot the data and the approximation\n",
    "XÃÇ = predict(p_trained)\n",
    "# Trained on noisy data vs real solution\n",
    "pl_trajectory = plot(t, transpose(XÃÇ), xlabel = \"time\", ylabel = \"population size\", color = :red,\n",
    "                     label = [\"UDE approximation\" nothing])\n",
    "scatter!(solution.t, transpose(Xdata), color = :black, label = [\"measurements\" nothing])\n",
    "\n",
    "# Ideal unknown interactions of the predictor\n",
    "YÃÑ = [-p_[2] * (XÃÇ[1, :] .* XÃÇ[2, :])'; p_[3] * (XÃÇ[1, :] .* XÃÇ[2, :])']\n",
    "# Neural network guess\n",
    "YÃÇ = U(XÃÇ, p_trained, st)[1]\n",
    "\n",
    "pl_reconstruction = plot(t, transpose(YÃÇ), xlabel = \"time\", ylabel = \"interaction terms\", color = :red,\n",
    "                         label = [\"UDE approximation\" nothing])\n",
    "plot!(t, transpose(YÃÑ), color = :black, label = [\"true interaction\" nothing])\n",
    "\n",
    "# Plot the error\n",
    "pl_reconstruction_error = plot(t, norm.(eachcol(YÃÑ - YÃÇ)), yaxis = :log, xlabel = \"time\",\n",
    "                               ylabel = \"L2-error\", label = nothing, color = :red)\n",
    "pl_missing = plot(pl_reconstruction, pl_reconstruction_error, layout = (2, 1))\n",
    "\n",
    "pl_overall = plot(pl_trajectory, pl_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a47ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can we get a symbolic representation for the missing equations?\n",
    "\n",
    "- We will use [DataDrivenDiffEq.jl](https://docs.sciml.ai/DataDrivenDiffEq/stable/). \n",
    "- To do this, we first generate a symbolic basis $\\Phi$ that represents the space of mechanistic functions we believe this neural network $NN_\\theta$ should map to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f44cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@variables u[1:2]\n",
    "b = polynomial_basis(u, 4)\n",
    "basis = Basis(b, u);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552bcd7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To assess the capability of the sparse regression, we look at 3 cases:\n",
    "\n",
    "    - Full problem: What if we trained no neural network and tried to automatically uncover the equations from the original noisy data? \\\n",
    "    $\\rightarrow$ Structural identification of dynamical systems (SINDy) \n",
    "    \n",
    "    $$ \\min_{\\phi} \\sum_i \\left(\\frac{u_{i + 1} - u_i}{\\Delta t_i} - \\phi \\cdot \\Phi(u_i)\\right)^2 + \\| \\phi \\|_1, $$\n",
    "    where, for example,\n",
    "    $$\n",
    "    \\Phi(u) = [1, \\text{üêá}, \\text{üê∫}, \\text{üêá}^2, \\text{üêá}\\text{üê∫}, \\text{üê∫}^2, \\dots, \\text{üê∫}^n, \\sin(\\text{üêá}), \\cos(\\text{üê∫}), \\dots]\n",
    "    $$\n",
    "    - Ideal problem: What if we use the **ideal right-hand side missing derivative function** $f(u)$?\n",
    "    \n",
    "    $$ \\min_{\\phi} \\sum_i \\left(f(u_i) - \\phi \\cdot \\Phi(u_i)\\right)^2 + \\| \\phi \\|_1$$\n",
    "    \n",
    "    - **NN problem: Do the symbolic regression directly on the function $NN(u)$, i.e. the trained learned neural network**\n",
    "    \n",
    "    $$ \\min_{\\phi} \\sum_i \\left(NN_\\theta(u_i) - \\phi \\cdot \\Phi(u_i)\\right)^2 + \\| \\phi \\|_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_problem = ContinuousDataDrivenProblem(Xdata, t) # noisy data\n",
    "# XÃÇ = predict(p_trained)\n",
    "ideal_problem = DirectDataDrivenProblem(XÃÇ, YÃÑ) # YÃÑ = [-p_[2] * (XÃÇ[1, :] .* XÃÇ[2, :])'; p_[3] * (XÃÇ[1, :] .* XÃÇ[2, :])']\n",
    "nn_problem = DirectDataDrivenProblem(XÃÇ, YÃÇ) # YÃÇ = U(XÃÇ, p_trained, st)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66449718",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Methods for sparse regression\n",
    "\n",
    "\n",
    "We will use the ADMM method, which requires we define a set of shrinking cutoff values Œª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Œª = exp10.(-3:0.01:3)\n",
    "opt = ADMM(Œª)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8950a8e",
   "metadata": {},
   "source": [
    "... but there are many more methods. Have a look at the [DataDrivenDiffEq.jl documentation](https://docs.sciml.ai/DataDrivenDiffEq/stable/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a7a5c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# options = DataDrivenCommonOptions(maxiters = 10_000,\n",
    "#                                   normalize = DataNormalization(ZScoreTransform),\n",
    "#                                   selector = bic, digits = 1,\n",
    "#                                   data_processing = DataProcessing(split = 0.9,\n",
    "#                                                                    batchsize = 30,\n",
    "#                                                                    shuffle = true,\n",
    "#                                                                    rng = StableRNG(1111)))\n",
    "\n",
    "# full_res = solve(full_problem, basis, opt, options = options)\n",
    "# full_eqs = get_basis(full_res)\n",
    "# println(full_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc7521",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "options = DataDrivenCommonOptions(maxiters = 10_000,\n",
    "                                  normalize = DataNormalization(ZScoreTransform),\n",
    "                                  selector = bic, digits = 1,\n",
    "                                  data_processing = DataProcessing(split = 0.9,\n",
    "                                                                   batchsize = 30,\n",
    "                                                                   shuffle = true,\n",
    "                                                                   rng = StableRNG(1111)))\n",
    "\n",
    "ideal_res = solve(ideal_problem, basis, opt, options = options)\n",
    "ideal_eqs = get_basis(ideal_res)\n",
    "println(ideal_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef074b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "options = DataDrivenCommonOptions(maxiters = 10_000,\n",
    "                                  normalize = DataNormalization(ZScoreTransform),\n",
    "                                  selector = bic, digits = 1,\n",
    "                                  data_processing = DataProcessing(split = 0.8,\n",
    "                                                                   batchsize = 30,\n",
    "                                                                   shuffle = true,\n",
    "                                                                   rng = StableRNG(1111)))\n",
    "\n",
    "nn_res = solve(nn_problem, basis, opt, options = options)\n",
    "nn_eqs = get_basis(nn_res)\n",
    "println(nn_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140687e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for eqs in (ideal_eqs, nn_eqs)\n",
    "    println(eqs)\n",
    "    println(get_parameter_map(eqs))\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d1acb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Do predictions using our discovered model \n",
    "\n",
    "To do so, we embed the basis into a function like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e23cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the recovered, hybrid model\n",
    "function recovered_dynamics!(du, u, p, t)\n",
    "    uÃÇ = nn_eqs(u, p) # Recovered equations\n",
    "    du[1] = p_[1] * u[1] + uÃÇ[1]\n",
    "    du[2] = -p_[4] * u[2] + uÃÇ[2]\n",
    "end\n",
    "\n",
    "estimation_prob = ODEProblem(recovered_dynamics!, u0, tspan, get_parameter_values(nn_eqs))\n",
    "est_solution = solve(estimation_prob, Tsit5(), saveat = solution.t)\n",
    "\n",
    "# Plot\n",
    "pl = plot(legend=true, xlabel=\"time\", ylabel=\"population size\")\n",
    "plot!(pl, solution.t, [u[1] for u in solution.u], color=:red, linewidth=2.5, style=:dash, label = \"true\")\n",
    "plot!(pl, solution.t, [u[2] for u in solution.u], color=:blue, linewidth=2.5, style=:dash, label = \"true\")\n",
    "plot!(pl, est_solution.t, [u[1] for u in est_solution.u], color=:red, linewidth=2.5, label = \"est\")\n",
    "plot!(pl, est_solution.t, [u[2] for u in est_solution.u], color=:blue, linewidth=2.5, label = \"est\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864897a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fine tune the parameters \n",
    "\n",
    "- minimizing the residuals between the UDE predictor and our recovered parametrized equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function parameter_loss(p)\n",
    "    Y = reduce(hcat, map(Base.Fix2(nn_eqs, p), eachcol(XÃÇ)))\n",
    "    sum(abs2, YÃÇ .- Y)\n",
    "end\n",
    "\n",
    "optf = Optimization.OptimizationFunction((x, p) -> parameter_loss(x), adtype)\n",
    "optprob = Optimization.OptimizationProblem(optf, get_parameter_values(nn_eqs))\n",
    "parameter_res = Optimization.solve(optprob, Optim.LBFGS(), maxiters = 1000)\n",
    "\n",
    "# Check how this extrapolates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b9a74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic optimal control\n",
    "\n",
    "### Task \n",
    "- control the population size of üê∫ and üêá\n",
    "\n",
    "### Stochastic predator-prey model\n",
    "\n",
    "- Model: Flemming Holtorf, Christopher Rackauckas, arXiv:2211.15652 \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "d\\text{üêá} &= (\\alpha \\text{üêá} - \\beta \\text{üêá} \\text{üê∫}) dt + \\eta \\text{üêá} \\circ dW_t \\\\\n",
    "d\\text{üê∫} &= (-\\delta \\text{üê∫} + \\gamma \\text{üêá} \\text{üê∫} - NN_\\theta(\\text{üêá},\\text{üê∫}) \\text{üê∫}) dt\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "- üêá represents the population of prey.\n",
    "- üê∫ represents the population of predators.\n",
    "- Œ±, Œ≤, Œ≥, and Œ¥ are parameters governing the interactions between the prey and predator populations.\n",
    "- $\\eta \\text{üêá} \\circ dW_t$ is a Brownian motion modeling the effect of presence/absence of a secondary predator species.\n",
    "- $NN_\\theta$ is the parametrized controller and confined to [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e2b40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the experimental parameter\n",
    "tspan = (0.0, 5.0)\n",
    "u0 = [1.0, 0.25] \n",
    "p_ = [1, 2, 2, 1, 0.25 * 0.1]\n",
    "\n",
    "# Define controller\n",
    "const C = Lux.Chain(Lux.Dense(2, 64, relu), Lux.Dense(64, 1, sigmoid))\n",
    "# Get the initial parameters and state variables of the model\n",
    "p, st = Lux.setup(rng, C)\n",
    "const _Cst = st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51a5d7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the UDE model\n",
    "function ude_drift!(du, u, p, t, p_true)\n",
    "    cÃÇ = C(u, p, _Cst)[1] # neural network prediction\n",
    "    üêá, üê∫ = u\n",
    "    Œ±, Œ≤, Œ≥, Œ¥ = p_true\n",
    "    du[1] = Œ± * üêá - Œ≤ * üêá * üê∫\n",
    "    du[2] = -Œ¥ * üê∫ + Œ≥ * üêá * üê∫ - cÃÇ[1] * üê∫\n",
    "end\n",
    "\n",
    "function ude_diffusion!(du, u, p, t, p_true)\n",
    "    üêá, üê∫ = u\n",
    "    Œ∑ = p_true[end]\n",
    "    du[1] = Œ∑ * üêá\n",
    "    du[2] = 0\n",
    "end\n",
    "\n",
    "# Closure with the known parameter\n",
    "ude_drift!(du, u, p, t) = ude_drift!(du, u, p, t, p_)\n",
    "ude_diffusion!(du, u, p, t) = ude_diffusion!(du, u, p, t, p_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e1d29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Randomly initialized controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92578f9",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define the problem\n",
    "prob_nn = SDEProblem(ude_drift!, ude_diffusion!, u0, tspan, p)\n",
    "prob = EnsembleProblem(prob_nn)\n",
    "\n",
    "sol = solve(\n",
    "    EnsembleProblem(SDEProblem(ude_drift!, ude_diffusion!, u0, (0, 10), p)),\n",
    "    EulerHeun(), dt=0.01, trajectories=100)\n",
    "\n",
    "function plot_ensemble(sol)\n",
    "    pl = plot(legend=false, xlabel=\"time\", ylabel=\"population size\")\n",
    "    for s in sol\n",
    "        plot!(pl, s.t, [u[1] for u in s.u], color=:red, linewidth=0.5)\n",
    "        plot!(pl, s.t, [u[2] for u in s.u], color=:blue, linewidth=0.5)\n",
    "    end\n",
    "    plot!(pl, [tspan[1], 10], [0.75, 0.75], style=:dash, color=:black, linewidth=2.5)\n",
    "    plot!(pl, [tspan[1], 10], [0.5, 0.5], style=:dash, color=:black, linewidth=2.5)\n",
    "    pl\n",
    "end\n",
    "\n",
    "plot_ensemble(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943cdbc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choose stage cost\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "l(\\text{üêá},\\text{üê∫}, NN_\\theta) &= (\\text{üêá} - 0.75)^2 + (\\text{üê∫} - 0.5)^2 + \\frac{(NN_\\theta(\\text{üêá}, \\text{üê∫}) - 0.5)^2}{5}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4badecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the training loop\n",
    "function predict(Œ∏)\n",
    "    _prob = remake(prob_nn, p=Œ∏)\n",
    "    sol = solve(EnsembleProblem(_prob), EulerHeun(), dt=0.01, sensealg=BacksolveAdjoint(autojacvec=ReverseDiffVJP()), trajectories=10)\n",
    "    A = convert(Array, sol)\n",
    "end\n",
    "\n",
    "function loss(Œ∏)\n",
    "    XÃÇ = predict(Œ∏)\n",
    "\n",
    "    # dim = (#time steps, #trajectories)\n",
    "    cs = @view (C(XÃÇ, Œ∏, _Cst)[1])[1, :, :]\n",
    "    üêás = @view XÃÇ[1, :, :]\n",
    "    üê∫s = @view XÃÇ[2, :, :]\n",
    "\n",
    "    # stagecost\n",
    "    sum((üêás .- 0.75) .^ 2 + (üê∫s .- 0.5) .^ 2 + (cs .- 0.5) .^ 2 / 5)/10\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d63e7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Run the training process!\n",
    "losses = Float64[]\n",
    "\n",
    "callback = function (p, l)\n",
    "    push!(losses, l)\n",
    "    if length(losses) % 50 == 0\n",
    "        sol = solve(\n",
    "            EnsembleProblem(SDEProblem(ude_drift!, ude_diffusion!, u0, (0, 10), p)),\n",
    "            EulerHeun(), dt=0.01, trajectories=100)\n",
    "        display(plot_ensemble(sol))\n",
    "        println(\"Current loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "    end\n",
    "    return false\n",
    "end\n",
    "\n",
    "adtype = Optimization.AutoZygote()\n",
    "optf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)\n",
    "optprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e5e74",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimized controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be27103",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = Optimization.solve(optprob, ADAM(), callback=callback, maxiters=500)\n",
    "println(\"Training loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "\n",
    "\n",
    "sol = solve(\n",
    "    EnsembleProblem(SDEProblem(ude_drift!, ude_diffusion!, u0, (0,10), res.u)),\n",
    "    EulerHeun(), dt=0.01, trajectories=100)\n",
    "\n",
    "plot_ensemble(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8929fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Current research and developments\n",
    "\n",
    "## I) Shadowing methods for chaotic systems\n",
    "\n",
    "- Compute derivatives for long-time averaged quantities of solutions to chaotic systems\n",
    "- Lorenz system\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dx}{dt} &= 10(y - x) \\\\\n",
    "\\frac{dy}{dt} &= x(\\rho - z) - y \\\\\n",
    "\\frac{dz}{dt} &= xy - \\frac{8}{3}z\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da78517",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# simulate a trajectory of the Lorenz system forward\n",
    "function lorenz!(du,u,p,t)\n",
    "  du[1] = 10*(u[2]-u[1])\n",
    "  du[2] = u[1]*(p[1]-u[3]) - u[2]\n",
    "  du[3] = u[1]*u[2] - (8//3)*u[3]\n",
    "end\n",
    "\n",
    "p = [28.0]\n",
    "tspan_init = (0.0, 30.0)\n",
    "tspan_attractor = (30.0, 50.0)\n",
    "u0 = [0.1, 0.5, 0.4]\n",
    "prob_init = ODEProblem(lorenz!, u0, tspan_init, p)\n",
    "sol_init = solve(prob_init, Tsit5())\n",
    "prob_attractor = ODEProblem(lorenz!, sol_init[end], tspan_attractor, p)\n",
    "sol_attractor = solve(prob_attractor, Vern9(), abstol=1e-14, reltol=1e-14)\n",
    "\n",
    "pl1 = plot(sol_attractor, idxs=(1,2,3), legend=true,\n",
    "  label=false,\n",
    "  labelfontsize=20,\n",
    "  lw=2,\n",
    "  xlabel=L\"x\", ylabel=L\"y\", zlabel=L\"z\",\n",
    "  xlims=(-25,30),ylims=(-30,30),zlims=(5,49)\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9fac8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## \"Butterfly effect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f308996",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "prob_attractor1 = ODEProblem(lorenz!, sol_init[end], (0.0, 20.0), p)\n",
    "prob_attractor2 = ODEProblem(lorenz!, convert.(Float32, sol_init[end]), (0f0, 20f0), convert.(Float32,p))\n",
    "sol1 = solve(prob_attractor1, Tsit5(), abstol=1e-6, reltol=1e-6, saveat=0.01)\n",
    "sol2 = solve(prob_attractor2, Tsit5(), abstol=1f-6, reltol=1f-6, saveat=0.01f0)\n",
    "\n",
    "plt1 = plot(sol1, idxs=(1,2,3), denseplot=true, legend=true,\n",
    "     label=\"Float64\", labelfontsize=20, lw=2,\n",
    "     xlabel=L\"x\", ylabel=L\"y\", zlabel=L\"z\",\n",
    "     xlims=(-20,25),ylims=(-28,25),zlims=(5,48))\n",
    "plot!(plt1, sol2,idxs=(1,2,3), denseplot=true, label=\"Float32\",\n",
    "     xlims=(-20,25),ylims=(-28,25),zlims=(5,48))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b22e2e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Long-time averaged quantities\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "      \\langle g(\\theta) \\rangle_\\infty = \\lim_{T \\rightarrow \\infty} \\langle g(\\theta)\\rangle_T,\n",
    "\\end{align*}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "      \\langle g(\\theta) \\rangle_T = \\frac 1 T \\int_0^T g(u(t),\\theta)~ dt,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "- Ergodicity $\\rightarrow$ $\\langle g(\\theta) \\rangle_\\infty$ depends only on $\\theta$.\n",
    "- Consider $\\langle z \\rangle_T$ for large $T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03813f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "function compute_objective(sol)\n",
    "    quadgk(t -> sol(t)[end]/(tspan_attractor[2]-tspan_attractor[1]), tspan_attractor[1], tspan_attractor[2], atol=1e-14, rtol=1e-10)[1]\n",
    "end\n",
    "\n",
    "pl2 = plot(sol_attractor.t, getindex.(sol_attractor.u,3), ylabel=L\"z(t)\", xlabel=L\"t\", label=false, labelfontsize=20, lw = 2)\n",
    "mean_z = [mean(getindex.(sol_attractor.u,3))]\n",
    "int_z = compute_objective(sol_attractor)\n",
    "hline!(pl2, [int_z], label=L\"\\langle z\\rangle\", lw = 2)\n",
    "# for each value of the parameter, solve 20 times the initial value problem\n",
    "# wrap the procedure inside a function depending on p\n",
    "function Lorenz_solve(p)\n",
    "    u0 = rand(3)\n",
    "    prob_init = ODEProblem(lorenz!, u0, tspan_init, p)\n",
    "    sol_init = solve(prob_init, Tsit5())\n",
    "    prob_attractor = ODEProblem(lorenz!, sol_init[end], tspan_attractor, p)\n",
    "    sol_attractor = solve(prob_attractor, Vern9(), abstol=1e-14, reltol=1e-14)\n",
    "    sol_attractor, prob_attractor\n",
    "end\n",
    "\n",
    "Niter = 5\n",
    "ps = collect(10.0:1.0:50.0)\n",
    "probs = []\n",
    "sols = []\n",
    "zmean = []\n",
    "zstd = []\n",
    "for œÅ in ps\n",
    "  ztmp = []\n",
    "  for i=1:Niter\n",
    "    sol, prob = Lorenz_solve([œÅ])\n",
    "    zbar = compute_objective(sol)\n",
    "    push!(sols, sol)\n",
    "    push!(probs, prob)\n",
    "    push!(ztmp, zbar)\n",
    "  end\n",
    "  push!(zmean, mean(ztmp))\n",
    "  push!(zstd, std(ztmp))\n",
    "end\n",
    "\n",
    "pl3 = plot(ps, zmean, ribbon=zstd, ylabel=L\"\\langle z\\rangle\", xlabel=L\"\\rho\", legend=false, labelfontsize=20, lw = 2)\n",
    "pl4 = plot(pl2, pl3, margin=3Plots.mm, layout=(1, 2), size=(600, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfeb003",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## AD and finite-differences fail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb690d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function G(p, prob=prob_attractor)\n",
    "  tmp_prob = remake(prob, p=p)\n",
    "  tmp_sol = solve(tmp_prob, Vern9(), abstol=1e-14, reltol=1e-14)\n",
    "  res = compute_objective(tmp_sol)\n",
    "  @info res\n",
    "  res\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990da24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.gradient(G, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FiniteDiff.finite_difference_gradient(G, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00363b0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ... but shadowing methods do work!\n",
    "\n",
    "$\\rightarrow$ Check my blog post on \"[Shadowing Methods for Forward and Adjoint Sensitivity Analysis of Chaotic Systems](https://frankschae.github.io/post/shadowing/)\" for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30946bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective\n",
    "g(u,p,t) = u[end]\n",
    "\n",
    "# Least squares shadowing\n",
    "lss_problem = ForwardLSSProblem(sol_attractor, ForwardLSS(g = g))\n",
    "@show shadow_forward(lss_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0571af6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## II) StochasticAD for programs with discrete randomness\n",
    "\n",
    "[StochasticAD.jl](https://github.com/gaurav-arya/StochasticAD.jl) provides a smoothed perturbation analysis (SPA) estimator\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta} \\mathbb E[f(X(\\theta))]= \\mathbb E\\left[\\delta + w(\\theta) \\left(f(Y(\\theta)) - f(X(\\theta))\\right)\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "[Arya *et al*., Advances in Neural Information Processing Systems 35, 10435 (2022)]\\\n",
    "[Arya *et al*., ICML 2023 Workshop on Differentiable Almost Everything (2023)]\\\n",
    "[Fu *et al*., Conditional Monte Carlo: Gradient Estimation and Optimization Applications (1997)] \\\n",
    "[Heidergott and V√°zquez-Abad, Journal of Optimization Theory and Applications, 136(2):187‚Äì209 (2008)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0845fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using StochasticAD, Distributions\n",
    "f(p) = rand(Bernoulli(p)) # 1 with probability p, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07725b00",
   "metadata": {},
   "outputs": [],
   "source": [
    " st ochastic_triple(f, 0.5) #  Feeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e2911",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "function f(p)\n",
    "    a = p * (1 - p)\n",
    "    b = rand(Binomial(10, p))\n",
    "    c = 2 * b + 3 * rand(Bernoulli(p))\n",
    "    return a * c * rand(Normal(b, a))\n",
    "end\n",
    "\n",
    "st = @show stochastic_triple(f, 0.6) # sample a single stochastic triple at p = 0.6\n",
    "@show derivative_contribution(st) # which produces a single derivative estimate...\n",
    "\n",
    "samples = [derivative_estimate(f, 0.6) for i in 1:1000] # many samples from derivative program\n",
    "derivative = mean(samples)\n",
    "uncertainty = std(samples) / sqrt(1000)\n",
    "println(\"derivative of ùîº[X(p)] = $derivative ¬± $uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdcfff3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SciML's contributions\n",
    "\n",
    "SciML\n",
    "\n",
    "- offers top-tier solvers for various problems\n",
    "- allows one to seamlessly combine scientific models with data-driven machine learning approaches\n",
    "- provides workflows for parameter inference, model selection & discovery, stochastic optimal control, and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7254398",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define the experimental parameters\n",
    "tspan = (0.0, 5.0) # time span\n",
    "u0 = [3.0, 1.5] # initial condition\n",
    "p_ = [1.3, 0.9, 0.8, 1.8] # parameters\n",
    "\n",
    "p0 = [0.8, 0.3, 1.4, 1.3]\n",
    "\n",
    "# Define ODE problem\n",
    "prob = ODEProblem(lotka, u0, tspan, p_)\n",
    "solution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat=0.25)\n",
    "\n",
    "# Add noise in terms of the mean\n",
    "X = Array(solution)\n",
    "t = solution.t\n",
    "\n",
    "xÃÑ = mean(X, dims=2)\n",
    "noise_magnitude = 5e-3\n",
    "X‚Çô = (X .+ (noise_magnitude * xÃÑ) .* randn(rng, eltype(X), size(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e173837",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define a predictor\n",
    "species = 1\n",
    "function predict(Œ∏)\n",
    "    _prob = remake(prob, p=Œ∏)\n",
    "    Array(solve(_prob, Vern7(), saveat=t, abstol=1e-6, reltol=1e-6, sensealg=ForwardDiffSensitivity()))[species,:]\n",
    "end\n",
    "\n",
    "# Simple L2 loss\n",
    "function loss(Œ∏)\n",
    "    XÃÇ = predict(Œ∏)\n",
    "    sum(abs2, X‚Çô[species, :] .- XÃÇ)\n",
    "end\n",
    "\n",
    "loss(p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adec65",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "adtype = Optimization.AutoZygote()\n",
    "optf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)\n",
    "optprob = Optimization.OptimizationProblem(optf, p0)\n",
    "\n",
    "# Train with Adam\n",
    "res1 = Optimization.solve(optprob, ADAM(), maxiters=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73cd80c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "loss(res1.u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cfde4b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "rise": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
